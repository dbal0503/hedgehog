{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Checkpoint Landscape Probe Experiment\n",
    "\n",
    "**Question:** Do landscape probes measured during pretraining (under MLM loss) predict which fine-tuning hyperparameters work best?\n",
    "\n",
    "**Design:** Load MultiBERTs checkpoints at different pretraining stages, measure landscape probes under MLM loss, then fine-tune each on SST-2 with a hyperparameter grid. Correlate pretrain probes with fine-tuning outcomes.\n",
    "\n",
    "**Quick test:** 1 checkpoint (step_2000k) x 2 seeds x 16 HP configs = 32 fine-tuning runs (~1 hour on T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install torch transformers datasets scipy pandas matplotlib seaborn tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (uncomment if running in Colab)\n",
    "# !git clone https://github.com/<your-username>/hedgehog.git\n",
    "# %cd hedgehog/landscape-probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# If running from the landscape-probes directory\n",
    "if os.path.basename(os.getcwd()) != 'landscape-probes':\n",
    "    # Try to find it\n",
    "    for candidate in ['landscape-probes', 'hedgehog/landscape-probes']:\n",
    "        if os.path.isdir(candidate):\n",
    "            os.chdir(candidate)\n",
    "            break\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import experiment modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probes.landscape_probes import LandscapeProbes\n",
    "from experiments.pretrain_probe_sweep import (\n",
    "    MLMCriterion,\n",
    "    load_mlm_data,\n",
    "    load_finetune_data,\n",
    "    measure_pretrain_probes,\n",
    "    finetune_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Quick test: 1 checkpoint x 2 seeds = 2 pretrained models, each fine-tuned with 16 HP configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick test config ---\n",
    "CHECKPOINT_STEPS = [\"step_2000k\"]  # Just the final checkpoint\n",
    "SEEDS = [0, 1]                      # 2 seeds\n",
    "\n",
    "# --- Full experiment (uncomment when ready) ---\n",
    "# CHECKPOINT_STEPS = [\"step_200k\", \"step_600k\", \"step_1000k\", \"step_2000k\"]\n",
    "# SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "# HP grid (same as original experiments)\n",
    "LEARNING_RATES = [1e-5, 2e-5, 5e-5, 1e-4]\n",
    "WARMUP_RATIOS = [0.0, 0.1]\n",
    "WEIGHT_DECAYS = [0.0, 0.01]\n",
    "\n",
    "TASK = \"sst2\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"./results_pretrain\"\n",
    "N_HUTCHINSON = 50\n",
    "\n",
    "n_hp = len(LEARNING_RATES) * len(WARMUP_RATIOS) * len(WEIGHT_DECAYS)\n",
    "n_checkpoints = len(SEEDS) * len(CHECKPOINT_STEPS)\n",
    "total_runs = n_checkpoints * n_hp\n",
    "\n",
    "print(f\"Checkpoints: {CHECKPOINT_STEPS}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"HP configs: {n_hp}\")\n",
    "print(f\"Total fine-tuning runs: {total_runs}\")\n",
    "print(f\"Estimated time: ~{total_runs * 3 / 60:.1f} hours on T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load MLM data (WikiText-2) for pretrain probe measurement, and SST-2 for fine-tuning. This only needs to happen once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Loading MLM data (WikiText-2)...\")\n",
    "mlm_loader = load_mlm_data(tokenizer, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Loading {TASK} data...\")\n",
    "train_loader, val_loader, num_labels = load_finetune_data(\n",
    "    tokenizer, TASK, batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment\n",
    "\n",
    "For each checkpoint:\n",
    "1. Measure 6 landscape probes under MLM loss\n",
    "2. Fine-tune on SST-2 with each HP config\n",
    "3. Save results incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, \"pretrain_probe_results.json\")\n",
    "\n",
    "# Resume support: load existing results\n",
    "results = []\n",
    "completed = set()\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path) as f:\n",
    "        results = json.load(f)\n",
    "    for r in results:\n",
    "        completed.add((r['seed'], r['checkpoint_step'], r['lr'],\n",
    "                       r['warmup_ratio'], r['weight_decay']))\n",
    "    print(f\"Resuming: {len(completed)} runs already completed, {total_runs - len(completed)} remaining\")\n",
    "else:\n",
    "    print(f\"Starting fresh: {total_runs} runs to go\")\n",
    "\n",
    "pretrain_probes_cache = {}\n",
    "run_count = 0\n",
    "experiment_start = time.time()\n",
    "\n",
    "for seed in SEEDS:\n",
    "    for step in CHECKPOINT_STEPS:\n",
    "        checkpoint_name = f\"google/multiberts-seed_{seed}-{step}\"\n",
    "        cache_key = (seed, step)\n",
    "\n",
    "        # Skip if all HP configs done for this checkpoint\n",
    "        all_done = all(\n",
    "            (seed, step, lr, warmup, wd) in completed\n",
    "            for lr in LEARNING_RATES\n",
    "            for warmup in WARMUP_RATIOS\n",
    "            for wd in WEIGHT_DECAYS\n",
    "        )\n",
    "        if all_done:\n",
    "            run_count += n_hp\n",
    "            print(f\"Skipping {checkpoint_name} (all configs done)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Checkpoint: {checkpoint_name}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        # --- Measure pretrain probes (once per checkpoint) ---\n",
    "        if cache_key not in pretrain_probes_cache:\n",
    "            try:\n",
    "                probes = measure_pretrain_probes(\n",
    "                    checkpoint_name, mlm_loader, DEVICE,\n",
    "                    n_hutchinson_samples=N_HUTCHINSON,\n",
    "                )\n",
    "                pretrain_probes_cache[cache_key] = probes\n",
    "                print(f\"  Probes: grad_norm={probes['gradient_norm']:.4f}, \"\n",
    "                      f\"trace={probes['hutchinson_trace']:.4f}, \"\n",
    "                      f\"top_eig={probes['top_eigenvalue']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR measuring probes: {e}\")\n",
    "                pretrain_probes_cache[cache_key] = {\n",
    "                    k: float('nan') for k in [\n",
    "                        'gradient_norm', 'gradient_variance', 'sam_sharpness',\n",
    "                        'hutchinson_trace', 'top_eigenvalue', 'loss_at_init',\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "        # --- Fine-tune with each HP config ---\n",
    "        for lr in LEARNING_RATES:\n",
    "            for warmup in WARMUP_RATIOS:\n",
    "                for wd in WEIGHT_DECAYS:\n",
    "                    run_count += 1\n",
    "                    config_key = (seed, step, lr, warmup, wd)\n",
    "\n",
    "                    if config_key in completed:\n",
    "                        continue\n",
    "\n",
    "                    print(f\"  [{run_count}/{total_runs}] lr={lr}, warmup={warmup}, wd={wd}\", end=\" \")\n",
    "\n",
    "                    try:\n",
    "                        ft_result = finetune_checkpoint(\n",
    "                            checkpoint_name, train_loader, val_loader,\n",
    "                            num_labels, lr=lr, warmup_ratio=warmup,\n",
    "                            weight_decay=wd, num_epochs=NUM_EPOCHS,\n",
    "                            device=DEVICE,\n",
    "                        )\n",
    "                        print(f\"-> acc={ft_result['val_accuracy']:.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"-> ERROR: {e}\")\n",
    "                        ft_result = {\n",
    "                            'val_accuracy': float('nan'),\n",
    "                            'final_train_loss': float('nan'),\n",
    "                        }\n",
    "\n",
    "                    pretrain_probes = pretrain_probes_cache[cache_key]\n",
    "                    result = {\n",
    "                        'seed': seed,\n",
    "                        'checkpoint_step': step,\n",
    "                        'checkpoint_name': checkpoint_name,\n",
    "                        'task': TASK,\n",
    "                        'lr': lr,\n",
    "                        'warmup_ratio': warmup,\n",
    "                        'weight_decay': wd,\n",
    "                        'batch_size': BATCH_SIZE,\n",
    "                        'num_epochs': NUM_EPOCHS,\n",
    "                        'val_accuracy': ft_result['val_accuracy'],\n",
    "                        'final_train_loss': ft_result['final_train_loss'],\n",
    "                        'pretrain_gradient_norm': pretrain_probes['gradient_norm'],\n",
    "                        'pretrain_gradient_variance': pretrain_probes['gradient_variance'],\n",
    "                        'pretrain_sam_sharpness': pretrain_probes['sam_sharpness'],\n",
    "                        'pretrain_hutchinson_trace': pretrain_probes['hutchinson_trace'],\n",
    "                        'pretrain_top_eigenvalue': pretrain_probes['top_eigenvalue'],\n",
    "                        'pretrain_loss': pretrain_probes['loss_at_init'],\n",
    "                    }\n",
    "\n",
    "                    results.append(result)\n",
    "\n",
    "                    # Save incrementally\n",
    "                    with open(output_path, 'w') as f:\n",
    "                        json.dump(results, f, indent=2)\n",
    "\n",
    "elapsed = time.time() - experiment_start\n",
    "print(f\"\\nDone! {len(results)} runs in {elapsed / 60:.1f} minutes\")\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now check if pretrain probes correlate with fine-tuning outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(output_path) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"Loaded {len(df)} runs\")\n",
    "print(f\"Checkpoints: {sorted(df['checkpoint_step'].unique())}\")\n",
    "print(f\"Seeds: {sorted(df['seed'].unique())}\")\n",
    "print(f\"\\nAccuracy stats:\")\n",
    "print(df['val_accuracy'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "PROBES = [\n",
    "    'pretrain_gradient_norm',\n",
    "    'pretrain_gradient_variance',\n",
    "    'pretrain_sam_sharpness',\n",
    "    'pretrain_hutchinson_trace',\n",
    "    'pretrain_top_eigenvalue',\n",
    "    'pretrain_loss',\n",
    "]\n",
    "\n",
    "SHORT = {\n",
    "    'pretrain_gradient_norm': 'grad_norm',\n",
    "    'pretrain_gradient_variance': 'grad_var',\n",
    "    'pretrain_sam_sharpness': 'sam_sharp',\n",
    "    'pretrain_hutchinson_trace': 'hutch_trace',\n",
    "    'pretrain_top_eigenvalue': 'top_eig',\n",
    "    'pretrain_loss': 'mlm_loss',\n",
    "}\n",
    "\n",
    "\n",
    "def spearman(x, y):\n",
    "    valid = ~(np.isnan(x) | np.isnan(y))\n",
    "    if valid.sum() < 3:\n",
    "        return np.nan, np.nan, \"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        r, p = stats.spearmanr(x[valid], y[valid])\n",
    "    sig = \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "    return r, p, sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pretrain probes vs all fine-tuning accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlations across all {len(df)} runs:\\n\")\n",
    "print(f\"{'Probe':<20} {'r':>8} {'p':>10} {'sig':>4}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for probe in PROBES:\n",
    "    r, p, sig = spearman(df[probe].values, df['val_accuracy'].values)\n",
    "    print(f\"{SHORT[probe]:<20} {r:>8.3f} {p:>10.4f} {sig:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pretrain probes vs best achievable accuracy (per checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each (seed, checkpoint), find the best accuracy across HP configs\n",
    "best_acc = df.groupby(['seed', 'checkpoint_step']).agg({\n",
    "    'val_accuracy': 'max',\n",
    "    **{p: 'first' for p in PROBES},\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Pretrain probes vs best accuracy (N = {len(best_acc)}):\\n\")\n",
    "print(f\"{'Probe':<20} {'r':>8} {'p':>10} {'sig':>4}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for probe in PROBES:\n",
    "    r, p, sig = spearman(best_acc[probe].values, best_acc['val_accuracy'].values)\n",
    "    print(f\"{SHORT[probe]:<20} {r:>8.3f} {p:>10.4f} {sig:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pretrain probes vs optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each (seed, checkpoint), find the LR that gave best accuracy\n",
    "optimal_lr_df = df.loc[df.groupby(['seed', 'checkpoint_step'])['val_accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Pretrain probes vs optimal LR (N = {len(optimal_lr_df)}):\\n\")\n",
    "print(f\"{'Probe':<20} {'r':>8} {'p':>10} {'sig':>4}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for probe in PROBES:\n",
    "    r, p, sig = spearman(optimal_lr_df[probe].values, optimal_lr_df['lr'].values)\n",
    "    print(f\"{SHORT[probe]:<20} {r:>8.3f} {p:>10.4f} {sig:>4}\")\n",
    "\n",
    "print(f\"\\nOptimal LR per checkpoint:\")\n",
    "for _, row in optimal_lr_df.iterrows():\n",
    "    print(f\"  seed={row['seed']}, {row['checkpoint_step']}: \"\n",
    "          f\"lr={row['lr']:.0e}, acc={row['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, probe in enumerate(PROBES):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(\n",
    "        df[probe], df['val_accuracy'],\n",
    "        c=np.log10(df['lr']), cmap='viridis', alpha=0.6, s=40, edgecolors='gray',\n",
    "    )\n",
    "    r, p, sig = spearman(df[probe].values, df['val_accuracy'].values)\n",
    "    ax.set_title(f\"{SHORT[probe]}\\nr={r:.3f}{sig} (p={p:.3f})\", fontsize=11)\n",
    "    ax.set_xlabel(SHORT[probe])\n",
    "    ax.set_ylabel('Val Accuracy')\n",
    "\n",
    "plt.suptitle('Pretrain Probes vs Fine-tuning Accuracy (color = log10 LR)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "outcomes = ['val_accuracy', 'lr']\n",
    "probe_labels = [SHORT[p] for p in PROBES]\n",
    "\n",
    "corr_matrix = np.zeros((len(PROBES), len(outcomes)))\n",
    "annot = np.empty_like(corr_matrix, dtype=object)\n",
    "\n",
    "for i, probe in enumerate(PROBES):\n",
    "    for j, outcome in enumerate(outcomes):\n",
    "        r, p, sig = spearman(df[probe].values, df[outcome].values)\n",
    "        corr_matrix[i, j] = r if not np.isnan(r) else 0\n",
    "        annot[i, j] = f\"{r:.2f}{sig}\" if not np.isnan(r) else \"--\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    corr_matrix, annot=annot, fmt='', cmap='RdBu_r', center=0,\n",
    "    xticklabels=outcomes, yticklabels=probe_labels, ax=ax,\n",
    "    vmin=-1, vmax=1,\n",
    ")\n",
    "ax.set_title('Pretrain Probe Correlations\\n(* p<0.05, ** p<0.01)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy distribution by LR\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for lr in sorted(df['lr'].unique()):\n",
    "    subset = df[df['lr'] == lr]\n",
    "    ax.hist(subset['val_accuracy'], alpha=0.5, label=f'lr={lr:.0e}', bins=15)\n",
    "\n",
    "ax.set_xlabel('Validation Accuracy')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Accuracy Distribution by Learning Rate')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_corrs = []\n",
    "for probe in PROBES:\n",
    "    r_acc, p_acc, _ = spearman(df[probe].values, df['val_accuracy'].values)\n",
    "    all_corrs.append(('val_accuracy', probe, r_acc, p_acc))\n",
    "    r_lr, p_lr, _ = spearman(df[probe].values, df['lr'].values)\n",
    "    all_corrs.append(('lr', probe, r_lr, p_lr))\n",
    "\n",
    "all_corrs.sort(key=lambda x: abs(x[2]) if not np.isnan(x[2]) else 0, reverse=True)\n",
    "\n",
    "print(\"\\nTop correlations:\")\n",
    "for outcome, probe, r, p in all_corrs[:6]:\n",
    "    sig = \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "    print(f\"  {SHORT[probe]} <-> {outcome}: r={r:.3f} (p={p:.4f}) {sig}\")\n",
    "\n",
    "n_sig = sum(1 for _, _, r, p in all_corrs if p < 0.05 and not np.isnan(p))\n",
    "max_r = max((abs(r) for _, _, r, _ in all_corrs if not np.isnan(r)), default=0)\n",
    "\n",
    "print(f\"\\nMax |r|: {max_r:.3f}\")\n",
    "print(f\"Significant correlations: {n_sig}/{len(all_corrs)}\")\n",
    "\n",
    "if max_r > 0.5 and n_sig >= 2:\n",
    "    print(\"\\nVerdict: STRONG signal -- pretrain probes predict fine-tuning outcomes\")\n",
    "elif max_r > 0.3 or n_sig >= 1:\n",
    "    print(\"\\nVerdict: MODERATE signal -- worth scaling up to full experiment\")\n",
    "else:\n",
    "    print(\"\\nVerdict: WEAK/NO signal in quick test -- may need more checkpoints for variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "If the quick test shows signal, scale up by changing the config cell:\n",
    "\n",
    "```python\n",
    "CHECKPOINT_STEPS = [\"step_200k\", \"step_600k\", \"step_1000k\", \"step_2000k\"]\n",
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "This gives 20 checkpoints x 16 HP configs = 320 runs (~8-10 hours on T4).\n",
    "\n",
    "The experiment will automatically resume from where it left off thanks to the incremental save."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
